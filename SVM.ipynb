{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f064506d",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c34c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4994a",
   "metadata": {},
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ef85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/content/drive/My Drive/Colab Notebooks/PreProcessing/all_kindle_review.csv'\n",
    "data = pd.read_csv(file_path, delimiter=',', encoding='utf-8')\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)  # Show all columns\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6867a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_read = data.shape[0]\n",
    "print(f\"Number of rows read: {num_rows_read}\")\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['wc', '-l', file_path], capture_output=True, text=True)\n",
    "num_lines_file = int(result.stdout.split()[0]) -1\n",
    "print(f\"Number of lines in file (including header): {num_lines_file}\")\n",
    "\n",
    "if num_rows_read == num_lines_file:\n",
    "    print(\"All rows appear to have been read successfully.\")\n",
    "else:\n",
    "    print(\"There might be some rows that were not read.\")\n",
    "    print(f\"Difference: {num_lines_file - num_rows_read}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648e8ae",
   "metadata": {},
   "source": [
    "Preprocessing the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ratings into categories (e.g., binary classification for sentiment)\n",
    "def classify_sentiment(rating):\n",
    "  if rating is None:  # Check if the rating is None\n",
    "      return 1\n",
    "  try:\n",
    "    rating = int(rating)  # or float(rating) if ratings can be decimals\n",
    "  except ValueError:\n",
    "      return \"neutral\"\n",
    "  if rating <= 2:\n",
    "      return \"Negative\"\n",
    "  elif rating == 3:\n",
    "      return \"Neutral\"\n",
    "  else:\n",
    "      return \"Positive\"\n",
    "y = data['rating'].apply(classify_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510544b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric characters (punctuation, special symbols, etc.)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "# Apply the function to the 'reviewText' column\n",
    "data['tokens'] = data['reviewText'].apply(lambda x: clean_and_tokenize(str(x)))\n",
    "\n",
    "# Drop rows with missing target values if necessary\n",
    "data.dropna(subset=['rating', 'tokens'], inplace=True)\n",
    "\n",
    "# Convert the rating into binary sentiment (positive/negative)\n",
    "data['sentiment'] = data['rating'].apply(classify_sentiment)\n",
    "\n",
    "# Display the preprocessed data with tokens\n",
    "print(data[['reviewText', 'tokens', 'sentiment']].head())\n",
    "\n",
    "data.dropna(subset=['rating','tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a97738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961249c0",
   "metadata": {},
   "source": [
    "Feature Extraction using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a72c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "data['tokens'] = data['tokens'].astype(str)\n",
    "data['tokens'] = data['tokens'].apply(word_tokenize)\n",
    "\n",
    "# Train a Word2Vec model on the tokenized text\n",
    "word2vec_model = Word2Vec(sentences=data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get the average word2vec vector for a review\n",
    "def get_avg_word2vec(tokens, model, vector_size):\n",
    "    valid_words = [word for word in tokens if word in model.wv]\n",
    "    if len(valid_words) == 0:\n",
    "        return [0] * vector_size\n",
    "    return np.mean([model.wv[word] for word in valid_words], axis=0)\n",
    "\n",
    "# Apply the function to create word2vec feature vectors\n",
    "import numpy as np\n",
    "X = np.array([get_avg_word2vec(tokens, word2vec_model, 100) for tokens in data['tokens']])\n",
    "y = data['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde51b17",
   "metadata": {},
   "source": [
    "Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407a364",
   "metadata": {},
   "source": [
    "Train the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84567fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Standardize the feature vectors and train SVM\n",
    "svm_model = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507fd24",
   "metadata": {},
   "source": [
    "Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafab9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "#print(classification_report(y_test, y_pred, target_names=['Negative', 'Neutral', 'Positive']))\n",
    "\n",
    "# Evaluate the model performance\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f360f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba556130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "# Define a custom explainer\n",
    "class CustomSVMExplainer(shap.Explainer):\n",
    "    def __init__(self, model, feature_names):\n",
    "        # Call the parent class's constructor\n",
    "        super().__init__(model.predict, feature_names=feature_names)\n",
    "        self.model = model\n",
    "\n",
    "    def shap_values(self, X):\n",
    "        # Get the predictions for the input data\n",
    "        predictions = self.model.predict(X)\n",
    "\n",
    "        # Initialize an array to hold SHAP values\n",
    "        shap_values = np.zeros((X.shape[0], X.shape[1]))\n",
    "\n",
    "        # Loop through each feature to compute SHAP values\n",
    "        for i in range(X.shape[1]):\n",
    "            # Create a copy of the input data\n",
    "            X_background = X.copy()\n",
    "            for j in range(X.shape[0]):\n",
    "                # Change one feature value to compute its impact\n",
    "                original_value = X_background[j, i]\n",
    "\n",
    "                # Perturb the feature\n",
    "                X_background[j, i] = np.mean(X[:, i])  # Replace with the mean for simplicity\n",
    "\n",
    "                # Get the predictions for the perturbed data\n",
    "                new_prediction = self.model.predict(X_background[[j]])  # Pass a single sample to predict\n",
    "\n",
    "                # Calculate the SHAP value as the difference in prediction\n",
    "                # Corrected indentation here\n",
    "                shap_values[j, i] = new_prediction[0] - predictions[j]  # Access the first element of new_prediction\n",
    "\n",
    "                # Restore original value\n",
    "                X_background[j, i] = original_value\n",
    "\n",
    "        return shap_values\n",
    "\n",
    "# Example usage\n",
    "# Assuming `svm_model` is your trained SVM model and `X_train` is your training data\n",
    "feature_names = [f'Feature {i + 1}' for i in range(X_train.shape[1])]\n",
    "custom_explainer = CustomSVMExplainer(svm_model, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7782dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the training dataset (80:20 for splitting and training)\n",
    "num_train_rows, num_train_columns = X_train.shape\n",
    "print(f\"Training dataset - Number of rows: {num_train_rows}, Number of columns: {num_train_columns}\")\n",
    "\n",
    "# Check the shape of the testing dataset\n",
    "num_test_rows, num_test_columns = X_test.shape\n",
    "print(f\"Testing dataset - Number of rows: {num_test_rows}, Number of columns: {num_test_columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47367f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP values for your test data\n",
    "shap_values = custom_explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize SHAP values\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b349bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `X_train` is your original training data with 100 features\n",
    "\n",
    "# Access the SVC estimator within the pipeline and set probability to True\n",
    "svm_model.set_params(svc__probability=True)\n",
    "\n",
    "# You may need to refit the model after this change\n",
    "svm_model.fit(X_train, y_train)  # Assuming X_train and y_train are your training data\n",
    "\n",
    "# Define SHAP explainer with the model and preprocessed training data\n",
    "# Use the original training data (X_train) instead of the transformed data\n",
    "explainer = shap.KernelExplainer(svm_model.predict_proba, X_train)\n",
    "\n",
    "# Calculate SHAP values for the custom input\n",
    "# Make sure custom_input has 100 features and is preprocessed in the same way as X_train\n",
    "shap_values = explainer.shap_values(custom_input)\n",
    "\n",
    "# Visualize SHAP values\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], custom_input)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
