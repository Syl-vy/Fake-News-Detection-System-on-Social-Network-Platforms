{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf821fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!pip install plotly\n",
    "!pip install --upgrade nbformat\n",
    "!pip install nltk\n",
    "!pip install spacy # spaCy is an open-source software library for advanced natural language processing\n",
    "!pip install WordCloud\n",
    "!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\n",
    "!pip install jupyterthemes\n",
    "import nltk\n",
    "\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# import keras\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Dropout, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split  # Ensure this is imported\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/REVIEW/Pheme_dataset_balanced.csv')\n",
    "\n",
    "# Convert 'is_rumor' to numeric and drop NaN values\n",
    "df['is_rumor'] = pd.to_numeric(df['is_rumor'], errors='coerce')\n",
    "df = df.dropna()\n",
    "\n",
    "# Download stopwords from nltk if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Remove URLs and special characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    return text.lower()\n",
    "\n",
    "# Apply cleaning function to the 'text' column\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Preprocess function to remove stopwords and short words\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token not in stop_words and len(token) > 2:  # Keep words longer than 2 characters\n",
    "            result.append(token)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# Apply preprocess function to get cleaned text\n",
    "df['cleaned_text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Prepare labels\n",
    "labels = df['is_rumor'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output shape of the training data for verification\n",
    "print(f\"Training data shape: {X_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}, Labels shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "with open('/content/drive/My Drive/Colab Notebooks/REVIEW/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Create an embedding matrix for the words in our tokenizer\n",
    "embedding_dim = 100\n",
    "total_words = len(tokenizer.word_index) + 1  # Adding 1 for padding index (0)\n",
    "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < total_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, batch_size=64, validation_split=0.1, epochs=50)\n",
    "predictions_prob = model.predict(X_test)\n",
    "predictions = [1 if prob > 0.5 else 0 for prob in predictions_prob]\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Model Accuracy: \", accuracy)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "precision = precision_score(y_test, predictions)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1fe004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "prediction = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i].item() > 0.5:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)\n",
    "\n",
    "y_test_list = y_test.tolist()\n",
    "prediction = prediction[:len(y_test_list)]  # Ensure sizes match\n",
    "\n",
    "false_positives = []\n",
    "false_negatives = []\n",
    "\n",
    "for i in range(len(y_test_list)):\n",
    "    if prediction[i] == 1 and y_test_list[i] == 0:\n",
    "        false_positives.append((i, X_test[i]))\n",
    "    elif prediction[i] == 0 and y_test_list[i] == 1:\n",
    "        false_negatives.append((i, X_test[i]))\n",
    "\n",
    "# Print total counts\n",
    "num_false_positives = len(false_positives)\n",
    "num_false_negatives = len(false_negatives)\n",
    "\n",
    "print(f\"\\nTotal False Positives: {num_false_positives}\")\n",
    "print(f\"Total False Negatives: {num_false_negatives}\")\n",
    "\n",
    "# Plotting the results\n",
    "labels = ['False Positives', 'False Negatives']\n",
    "counts = [num_false_positives, num_false_negatives]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, counts, color=['blue', 'red'])\n",
    "plt.title('Count of False Positives and False Negatives')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Classification Error Type')\n",
    "plt.ylim(0, max(counts) + 100)  # Adjust ylim for better visibility\n",
    "offset = 10\n",
    "for i in range(len(counts)):\n",
    "    plt.text(i, counts[i] + offset, str(counts[i]), ha='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99547a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix and plot it\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plotting the confusion matrix using seaborn heatmap for better visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Rumor', 'Rumor'], yticklabels=['Not Rumor', 'Rumor'])\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
