{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c842cf",
   "metadata": {},
   "source": [
    "Bi-LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/REVIEW/Pheme_dataset_balanced.csv')\n",
    "df.value_counts()\n",
    "df['is_rumor'] = pd.to_numeric(df['is_rumor'], errors='coerce')\n",
    "df.isnull().sum()\n",
    "df = df.dropna()\n",
    "df.info()\n",
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a77a92",
   "metadata": {},
   "source": [
    "Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "df['clean_joined'] = df['text'].apply(lambda x: \" \".join(preprocess(x)))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "vectorizer.fit(df['clean_joined'])\n",
    "tfidf_matrix = vectorizer.transform(df['clean_joined'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "features_df = pd.DataFrame(feature_names, columns=[\"Features\"])\n",
    "print(features_df)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(feature_names)\n",
    "N = 10  \n",
    "sequences = []\n",
    "for row in tfidf_matrix:\n",
    "    top_feature_indices = row.toarray().argsort()[0, -N:][::-1]\n",
    "    top_feature_tokens = [feature_names[i] for i in top_feature_indices]\n",
    "    sequence = tokenizer.texts_to_sequences(top_feature_tokens)\n",
    "    sequence = [item for sublist in sequence for item in sublist]\n",
    "    sequences.append(sequence)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "padded_sequences = pad_sequences(sequences, maxlen=40, padding='post', truncating='post')\n",
    "df['clean'] = df['text'].apply(preprocess)\n",
    "df['text'][0]\n",
    "print(df['clean'][0])\n",
    "df.head()\n",
    "list_of_words = []\n",
    "for i in df.clean:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words\n",
    "df['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))\n",
    "plt.figure(figsize = (8, 8))\n",
    "sns.countplot(y = \"topic\", data = df)\n",
    "plt.figure(figsize = (8, 8))\n",
    "sns.countplot(y = \"is_rumor\", data = df)\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df['is_rumor'] == 1]['clean_joined'].astype(str)))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df['is_rumor'] == 0]['clean_joined'].astype(str)))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "maxlen = -1\n",
    "for doc in df.clean_joined:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is =\", maxlen)\n",
    "import plotly.express as px  \n",
    "word_counts = [len(nltk.word_tokenize(x)) for x in df.clean_joined]\n",
    "fig = px.histogram(x=word_counts, nbins=100)\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of Word Counts in News Articles\",\n",
    "    xaxis_title=\"Number of Words per Article\",\n",
    "    yaxis_title=\"Frequency\",\n",
    ")\n",
    "fig.show()\n",
    "padded_sequences = pad_sequences(sequences, maxlen=40, padding='post', truncating='post')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "embedding_index = {}\n",
    "with open('/content/drive/My Drive/Colab Notebooks/REVIEW/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "embedding_dim = 100\n",
    "total_words = len(tokenizer.word_index) + 1  \n",
    "embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < total_words:  # Ensure we stay within bounds\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc208a6",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df['is_rumor'] = pd.to_numeric(df['is_rumor'], errors='coerce').fillna(0).astype(int)\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, df['is_rumor'].astype(int), test_size=0.2)\n",
    "from keras.regularizers import l2\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, weights=[embedding_matrix], trainable=True))  # Set trainable=True\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False)))  # Set return_sequences=False\n",
    "model.add(Dropout(0.5))  # Dropout rate of 50%\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))   \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "y_train = np.asarray(y_train)\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.1, epochs=50)\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "prediction = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i].item() > 0.5:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf38811",
   "metadata": {},
   "source": [
    "Calculating the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "import pandas as pd\n",
    "y_test_list = y_test.dropna().tolist()  # Remove NaN values\n",
    "prediction = prediction[:len(y_test_list)]\n",
    "accuracy = accuracy_score(y_test_list, prediction)\n",
    "print(\"Model Accuracy : \", accuracy)\n",
    "print(classification_report(y_test_list, prediction))\n",
    "precision = precision_score(y_test_list, prediction)\n",
    "print(\"Precision:\", precision)\n",
    "f1 = f1_score(y_test_list, prediction)\n",
    "print(\"F1-Score:\", f1)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "pred = model.predict(x_test)\n",
    "prediction = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i].item() > 0.5:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab62a30",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "predictions = [1 if prob > 0.5 else 0 for prob in pred]\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Rumor\", \"Rumor\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix for Bi-LSTM Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7677c8fe",
   "metadata": {},
   "source": [
    "Identify False Positive and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57daf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = []\n",
    "false_negatives = []\n",
    "for i in range(len(y_test_list)):\n",
    "    if prediction[i] == 1 and y_test_list[i] == 0:\n",
    "        false_positives.append((i, x_test[i]))  \n",
    "    elif prediction[i] == 0 and y_test_list[i] == 1:\n",
    "        false_negatives.append((i, x_test[i]))  \n",
    "def remove_padding(sequence):\n",
    "    return sequence[sequence != 0]\n",
    "print(f\"\\nTotal False Positives: {len(false_positives)}\")\n",
    "print(f\"Total False Negatives: {len(false_negatives)}\")\n",
    "print(\"\\nFalse Positives:\")\n",
    "for index, seq in false_positives:\n",
    "    cleaned_sequence = remove_padding(seq)  \n",
    "    print(f\"Index: {index}, Predicted: {prediction[index]}, Actual: {y_test_list[index]}, Sequence: {cleaned_sequence}\")\n",
    "print(\"\\nFalse Negatives:\")\n",
    "for index, seq in false_negatives:\n",
    "    cleaned_sequence = remove_padding(seq)  \n",
    "    print(f\"Index: {index}, Predicted: {prediction[index]}, Actual: {y_test_list[index]}, Sequence: {cleaned_sequence}\")\n",
    "num_false_positives = len(false_positives)\n",
    "num_false_negatives = len(false_negatives)\n",
    "labels = ['False Positives', 'False Negatives']\n",
    "counts = [num_false_positives, num_false_negatives]\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, counts, color=['blue', 'red'])\n",
    "plt.title('Count of False Positives and False Negatives')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Classification Error Type')\n",
    "plt.ylim(0, max(counts) + 100)\n",
    "offset = 10\n",
    "for i in range(len(counts)):\n",
    "    plt.text(i, counts[i] + offset, str(counts[i]), ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99fb46",
   "metadata": {},
   "source": [
    "Generate random news articles for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_random_news(num_articles=5):\n",
    "    # Sample words to create random sentences\n",
    "    sample_words = [\"breaking\", \"news\", \"report\", \"claims\", \"study\", \"research\", \"findings\",\n",
    "                    \"investigation\", \"reveals\", \"analysis\", \"government\", \"officials\",\n",
    "                    \"says\", \"sources\", \"data\", \"evidence\", \"public\", \"concerns\", \"murder\",\n",
    "                    \"scandal\", \"robbery\", \"bribes\", \"declares\", \"war\"]\n",
    "\n",
    "    random_articles = []\n",
    "    for _ in range(num_articles):\n",
    "        sentence_length = random.randint(5, 15)\n",
    "        random_sentence = ' '.join(random.choices(sample_words, k=sentence_length))\n",
    "        random_articles.append(random_sentence)\n",
    "\n",
    "    return random_articles\n",
    "\n",
    "def predict_news(random_articles):\n",
    "    # Preprocess the articles\n",
    "    cleaned_articles = [preprocess(article) for article in random_articles]\n",
    "    cleaned_joined = [\" \".join(article) for article in cleaned_articles]\n",
    "    sequences = tokenizer.texts_to_sequences(cleaned_joined)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=40, padding='post', truncating='post')\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    predicted_labels = [1 if pred > 0.5 else 0 for pred in predictions]\n",
    "    return predicted_labels\n",
    "random_news = generate_random_news(num_articles=1500)\n",
    "predictions = predict_news(random_news)\n",
    "for article, prediction in zip(random_news, predictions):\n",
    "    print(f\"Article: {article}\\nPrediction: {'Real News' if prediction == 1 else 'Fake News'}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac86f4",
   "metadata": {},
   "source": [
    "Code for XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "explainer = shap.Explainer(model, x_train)  \n",
    "subset_x_test = x_test[:10]\n",
    "shap_values = explainer(subset_x_test)\n",
    "shap.summary_plot(shap_values, subset_x_test)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "feature_names = tokenizer.index_word\n",
    "num_features = min(len(feature_names), padded_sequences.shape[1])\n",
    "feature_names_list = ['Padding'] + [feature_names[i] for i in range(1, num_features)]\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=feature_names_list)\n",
    "mean_shap_values = np.abs(shap_df).mean(axis=0)\n",
    "sorted_feature_indices = mean_shap_values.sort_values(ascending=False).index[:20]  # Get top 20 important features\n",
    "top_feature_names = []\n",
    "top_importances = []\n",
    "print(\"Top Features shown in the SHAP summary plot with their importance:\")\n",
    "for feature in sorted_feature_indices:\n",
    "    if feature == 'Padding':\n",
    "        index = 0\n",
    "        importance_value = mean_shap_values[index]\n",
    "        top_feature_names.append('Padding')\n",
    "        top_importances.append(importance_value)\n",
    "        print(f\"Feature Index: {index}, Feature Name: Padding, Importance: {importance_value}\")\n",
    "    else:\n",
    "        feature_index = [k for k, v in feature_names.items() if v == feature]\n",
    "        if feature_index:\n",
    "            index = feature_index[0]\n",
    "            importance_value = mean_shap_values[index]\n",
    "            top_feature_names.append(feature)\n",
    "            top_importances.append(importance_value)\n",
    "            print(f\"Feature Index: {index}, Feature Name: {feature}, Importance: {importance_value}\")\n",
    "        else:\n",
    "            print(f\"Feature '{feature}' not found in feature names.\")\n",
    "\n",
    "def plot_shap_values(feature_names, importances):\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature Name': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "\n",
    "    shap_df = shap_df.sort_values(by='Importance', ascending=False)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature Name', data=shap_df)\n",
    "\n",
    "    plt.title('Features Shown in Summary Plot by SHAP')\n",
    "    plt.xlabel('Feature Impact on Prediction')\n",
    "    plt.ylabel('Feature Names')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_shap_values(top_feature_names, top_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c44b8",
   "metadata": {},
   "source": [
    "Contract Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aca566",
   "metadata": {},
   "outputs": [],
   "source": [
    "const FakeNewsDetection = artifacts.require(\"FakeNewsDetection\");\n",
    "\n",
    "module.exports = function(deployer){\n",
    "    deployer.deploy(FakeNewsDetection);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29297af4",
   "metadata": {},
   "source": [
    "Smart Contract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "// SPDX-License-Identifier: MIT\n",
    "pragma solidity ^0.5.16;\n",
    "\n",
    "contract FakeNewsDetection {\n",
    "    struct News {\n",
    "        uint id;\n",
    "        string content;\n",
    "        address submitter;\n",
    "        bool isVerified;\n",
    "        bool isFake;\n",
    "        uint credibilityPoints;\n",
    "    }\n",
    "\n",
    "    mapping(uint => News) public newsRecords;\n",
    "    mapping(address => uint) public userCredibility;\n",
    "    uint public newsCount;\n",
    "\n",
    "    event NewsSubmitted(uint id, string content, address indexed submitter);\n",
    "    event NewsVerified(uint id, bool isFake, address indexed verifier, uint credibilityPoints);\n",
    "\n",
    "    constructor() public{\n",
    "        newsCount = 0;\n",
    "    }\n",
    "\n",
    "    function submitNews(string memory _content) public {\n",
    "        newsCount++;\n",
    "        newsRecords[newsCount] = News(newsCount, _content, msg.sender, false, false, 0);\n",
    "        emit NewsSubmitted(newsCount, _content, msg.sender);\n",
    "    }\n",
    "\n",
    "    function verifyNews(uint _newsId, bool _isFake) public {\n",
    "        require(_newsId != 0 && _newsId <= newsCount, \"Invalid news ID\");\n",
    "        require(!newsRecords[_newsId].isVerified, \"News already verified\");\n",
    "        newsRecords[_newsId].isFake = _isFake;\n",
    "        newsRecords[_newsId].isVerified = true;\n",
    "        uint credibilityPoints = _isFake ? 10 : 5;\n",
    "        newsRecords[_newsId].credibilityPoints = credibilityPoints;\n",
    "        userCredibility[msg.sender] = userCredibility[msg.sender] + credibilityPoints;\n",
    "\n",
    "        emit NewsVerified(_newsId, _isFake, msg.sender, credibilityPoints);\n",
    "    }\n",
    "\n",
    "    function getNews(uint _newsId) public view returns (string memory, bool, bool, uint) {\n",
    "        require(_newsId != 0 && _newsId <= newsCount, \"Invalid news ID\");\n",
    "        News memory newsItem = newsRecords[_newsId];\n",
    "        return (newsItem.content, newsItem.isVerified, newsItem.isFake, newsItem.credibilityPoints);\n",
    "    }\n",
    "\n",
    "    function getUserCredibility(address _user) public view returns (uint) {\n",
    "        return userCredibility[_user];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bccec3",
   "metadata": {},
   "source": [
    "Contract Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "const FakeNewsDetection = artifacts.require('FakeNewsDetection');\n",
    "\n",
    "contract('FakeNewsDetection', (accounts) => {\n",
    "  let contractInstance;\n",
    "\n",
    "  before(async () => {\n",
    "    contractInstance = await FakeNewsDetection.deployed();\n",
    "  });\n",
    "\n",
    "  describe('Deployment', () => {\n",
    "    it('should deploy successfully', async () => {\n",
    "      assert.notEqual(contractInstance.address, undefined, 'Contract address is undefined');\n",
    "    });\n",
    "  });\n",
    "\n",
    "  describe('News Submission', () => {\n",
    "    it('should allow submitting news', async () => {\n",
    "      await contractInstance.submitNews('Breaking News!', { from: accounts[0] });\n",
    "      const news = await contractInstance.newsRecords(1);\n",
    "      assert.equal(news.content, 'Breaking News!', 'News content mismatch');\n",
    "    });\n",
    "  });\n",
    "  describe('News Verification', () => {\n",
    "    it('should allow a verifier to verify news as fake and award credibility points', async () => {\n",
    "      const tx = await contractInstance.verifyNews(1, true, { from: accounts[1] });\n",
    "\n",
    "      const news = await contractInstance.newsRecords(1);\n",
    "      expect(news.isVerified).to.be.true;\n",
    "      expect(news.isFake).to.be.true;\n",
    "      expect(news.credibilityPoints.toNumber()).to.equal(10);\n",
    "\n",
    "      const verifierCredibility = await contractInstance.getUserCredibility(accounts[1]);\n",
    "      expect(verifierCredibility.toNumber()).to.equal(10);\n",
    "\n",
    "      expect(tx.logs[0].event).to.equal('NewsVerified');\n",
    "      expect(tx.logs[0].args.isFake).to.be.true;\n",
    "    });\n",
    "\n",
    "    it('should allow a verifier to verify news as not fake and award credibility points', async () => {\n",
    "      await contractInstance.submitNews('Another news content', { from: accounts[0] });\n",
    "      const newsCount = await contractInstance.newsCount();\n",
    "      expect(newsCount.toNumber()).to.equal(2);\n",
    "\n",
    "      const tx = await contractInstance.verifyNews(2, false, { from: accounts[1] });\n",
    "\n",
    "      const news = await contractInstance.newsRecords(2);\n",
    "      expect(news.isVerified).to.be.true;\n",
    "      expect(news.isFake).to.be.false;\n",
    "      expect(news.credibilityPoints.toNumber()).to.equal(5);\n",
    "\n",
    "      const verifierCredibility = await contractInstance.getUserCredibility(accounts[1]);\n",
    "      expect(verifierCredibility.toNumber()).to.equal(15);\n",
    "\n",
    "      expect(tx.logs[0].event).to.equal('NewsVerified');\n",
    "      expect(tx.logs[0].args.isFake).to.be.false;\n",
    "    });\n",
    "  });\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085ce94",
   "metadata": {},
   "source": [
    "Web3.js code for interaction with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b4509",
   "metadata": {},
   "outputs": [],
   "source": [
    "const Web3 = require(\"web3\");\n",
    "const axios = require(\"axios\"); \n",
    "const contractABI = require(\"D:\\Studies\\Blockchain\\Truffle\\FakeNews\\FakeNews.json\").abi; \n",
    "const contractAddress = \"0xb06111b8AdDC6d1bdC7bF771F036472AbbfF67b1\"; \n",
    "const web3 = new Web3(\"http://127.0.0.1:7545\"); \n",
    "const contract = new web3.eth.Contract(contractABI, contractAddress);\n",
    "const account = \"0xb06111b8AdDC6d1bdC7bF771F036472AbbfF67b1\"; \n",
    "const mlApiUrl = \"http://127.0.0.1:5000/predict\";\n",
    "contract.events\n",
    "  .NewsSubmitted()\n",
    "  .on(\"data\", async (event) => {\n",
    "    console.log(\"News Submitted Event Detected:\", event);\n",
    "    const { newsId, content } = event.returnValues;\n",
    "    console.log(`Processing news with ID: ${newsId}, Content: ${content}`);\n",
    "    try {\n",
    "      const response = await axios.post(mlApiUrl, { text: content });\n",
    "      if (response.data && response.data.prediction !== undefined) {\n",
    "        const prediction = response.data.prediction; \n",
    "        console.log(`Prediction received: ${prediction ? \"Real\" : \"Fake\"}`);\n",
    "        await contract.methods\n",
    "          .verifyNews(newsId, prediction === 1) \n",
    "          .send({ from: account, gas: 300000 });\n",
    "        console.log(`News ID ${newsId} verification updated on the blockchain.`);\n",
    "      } else {\n",
    "        console.error(\"Invalid response from ML model:\", response.data);\n",
    "      }\n",
    "    } catch (error) {\n",
    "      console.error(\"Error processing news with ML model:\", error.message);\n",
    "    }\n",
    "  })\n",
    "  .on(\"error\", (error) => {\n",
    "    console.error(\"Error listening to NewsSubmitted events:\", error.message);\n",
    "  });\n",
    "console.log(\"Listening for NewsSubmitted events...\");\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
